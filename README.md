# Xilo AI Tutor - Large v5 Overhaul

**Version 5.09.5** (Frontend + Lessons Release)

An advanced AI tutoring system with modular model architecture, powered by GPT-OSS 20B and Llama 3.1 8B, optimized for Intel Arc GPU with Vulkan acceleration.

> **Milestone Release**: Complete frontend rebuild with working lessons system, split-view interface, and AI-powered quiz flow.

## Branches

**main**: Stable production branch  
**v5.09.5**: Frontend + lessons system (current)
**v5.9-backend-only**: Backend-only branch (previous)
**latest**: Development branch

## ğŸš€ Features (v5.09.5)

### Core Features
- **Modular Model Architecture**: GPT-OSS 20B (reasoning model) with Llama 3.1 8B support
- **Vulkan GPU Acceleration**: llama.cpp with Vulkan backend for Intel Arc GPUs
- **Reasoning Model Support**: Automatic extraction of final answers from chain-of-thought models
- **Multilingual Support**: 13 languages (English + 12 Indian languages) with translation toggle

### Lesson System
- **3-Phase Learning Flow**: 
  1. **Explanation**: Read content with doubt chat available
  2. **Assessment**: Quiz with AI-powered hints
  3. **Completion**: Progress tracking and stats
- **Split-View Interface**: Content/quiz on left (60%), doubt chat on right (40%)
- **AI-Powered Hints**: Context-aware hints based on student's specific answer
- **Smart Quiz Closure**: Automatically closes quiz if answer is completely unrelated
- **Progress Tracking**: Resume lessons from last incomplete section (localStorage + backend)
- **Auto-Generated Hints**: No hint button - AI generates feedback immediately after wrong answers

### Answer Evaluation
- **3-Tier Hint System**:
  1. Spelling check (80% similarity) â†’ "Check your spelling"
  2. AI categorization (related answer) â†’ Contextual hint without revealing answer
  3. Unrelated answer â†’ Close quiz, force material review
- **Max Attempts**: 3 attempts per question with adaptive feedback

### Frontend
- **Modern Light Theme**: Excellent readability, clean design
- **Responsive Design**: Works on desktop and mobile
- **Language Selector**: 13 languages with deep/AI translation toggle
- **Chat Interface**: Working chat page with message history
- **Lessons Grid**: Grade/subject filters with lesson cards
- **Split-View Modal**: Full-screen lesson viewer with doubt chat

## ğŸ—ï¸ Architecture

- **Backend**: Flask + APIFlask with llama.cpp server integration
- **Frontend**: HTML5/CSS3/JavaScript with modal-based lesson viewer
- **AI Models** (Modular System): 
  - **GPT-OSS 20B** (Active): Reasoning model with chain-of-thought extraction
  - **Llama 3.1 8B Instruct** (Ready): Direct instruction-following model
  - **Mistral 7B** (Optional): Available for evaluation tasks
- **Hardware**: Intel Arc GPUs (B580 recommended: 12GB VRAM + 8GB shared = 20GB total)
- **Acceleration**: llama.cpp with Vulkan backend (GPU offloading)

## ğŸ“ Project Structure

```
xilov5/
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ models/                         # Model implementations
â”‚   â”œâ”€â”€ gptoss_model.py            # GPT-OSS 20B (active)
â”‚   â”œâ”€â”€ llama_model.py             # Llama 3.1 8B
â”‚   â””â”€â”€ mistral_model.py           # Mistral 7B
â”œâ”€â”€ utils/                          # Utilities
â”‚   â”œâ”€â”€ lesson_manager.py          # Lesson loading/management
â”‚   â”œâ”€â”€ answer_evaluator.py        # 3-tier hint system
â”‚   â”œâ”€â”€ chat_memory.py             # Conversation history
â”‚   â””â”€â”€ language_manager.py        # Multilingual support
â”œâ”€â”€ templates/                      # HTML templates
â”‚   â”œâ”€â”€ index.html                 # Chat page
â”‚   â””â”€â”€ lessons.html               # Lessons page (split-view)
â”œâ”€â”€ static/                         # Frontend assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ base.css               # Design system (light theme)
â”‚   â”‚   â”œâ”€â”€ chat.css               # Chat interface styles
â”‚   â”‚   â””â”€â”€ lessons.css            # Lessons interface styles
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ chat.js                # Chat functionality
â”‚       â””â”€â”€ lessons.js             # Lessons functionality (500+ lines)
â””â”€â”€ lessons/                        # Lesson content (JSON)
    â”œâ”€â”€ grade_5/
    â”‚   â”œâ”€â”€ english/
    â”‚   â”‚   â””â”€â”€ poetry_basics.json
    â”‚   â”œâ”€â”€ math/
    â”‚   â””â”€â”€ science/
    â””â”€â”€ metadata.json
```

## âš ï¸ SYSTEM REQUIREMENTS

### âš¡ GPU Acceleration
**This software uses Vulkan for GPU acceleration. Intel Arc GPUs recommended, but any Vulkan-compatible GPU may work.**

### Hardware Requirements
- **Intel Arc GPU**: Battlemage B580 (12GB + 8GB shared = 20GB total) or A770 recommended
- **Vulkan Support**: Required for GPU acceleration  
- **GPU Memory**: 12-20GB total VRAM (for GPT-OSS 20B)
- **System RAM**: 16GB minimum (32GB recommended)
- **Storage**: 20GB free space (for model files)

### Software Requirements
- **Operating System**: Windows 10/11 (64-bit)
- **Python**: 3.11 recommended (3.8-3.11 supported)
- **Intel GPU Drivers**: Latest version from Intel
- **llama.cpp**: Installed via winget (ggml.llamacpp)
- **Vulkan Runtime**: Included with llama.cpp from winget

### Compatibility Notes
- **Intel Arc GPU**: Fully supported with Vulkan backend
- **NVIDIA GPUs**: May work with Vulkan (not tested)
- **AMD GPUs**: May work with Vulkan (not tested)
- **Intel Integrated Graphics**: Not recommended (insufficient VRAM)
- **CPU-only execution**: Possible but extremely slow (~0.5 tok/s)

## ğŸ› ï¸ Installation

### Prerequisites
```powershell
# 1. Install Intel GPU drivers (latest from Intel website)

# 2. Install llama.cpp with Vulkan support via winget
winget install ggml.llamacpp

# 3. Verify llama-server.exe is installed
$env:LOCALAPPDATA + "\Microsoft\WinGet\Packages\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\llama-server.exe"
```

### Dependencies
```bash
# Install Python dependencies
pip install flask apiflask requests deep_translator

# Optional: For debugging/monitoring
pip install psutil
```

### Model Setup
**GPT-OSS 20B** (Current):
- Download via AI Playground or manually from HuggingFace
- Default location: `C:\Users\{user}\AppData\Local\Programs\AI Playground\resources\service\models\llm\ggufLLM\unsloth---gpt-oss-20b-GGUF\gpt-oss-20b-Q8_0.gguf`
- Size: ~13.9GB

**Llama 3.1 8B** (Alternative):
- Download GGUF from HuggingFace: `bartowski/Meta-Llama-3.1-8B-Instruct-GGUF`
- Update path in `models/llama31_model.py` line 31
- Size: ~8.5GB (Q8_0)

## ğŸš€ Usage

### Starting the Server
```bash
python app.py
```

### Accessing the Interface
Open your browser and navigate to:
```
http://localhost:5000
```

### Features
- **Lesson Browser**: Browse lessons by grade (5-12) and subject
- **3-Phase Learning**: 
  1. Explanation with reading material
  2. Doubt clearing chat (multilingual, 13 languages)
  3. Assessment with AI-powered hints
- **Progress Tracking**: Automatic progress saving per user
- **Answer Evaluation**: GPT-OSS 20B provides reasoning-based feedback and hints
- **Reasoning Model Support**: Automatic extraction of final answers from chain-of-thought

## ğŸ“Š Performance Metrics

### Intel Arc B580 Performance (GPT-OSS 20B + Vulkan)
- **Model Loading**: 48-60 seconds (13.9GB model)
- **VRAM Usage**: 13.9GB (GPT-OSS 20B)
- **Generation Speed**: 3.7-7.0 tok/s (varies with context length)
- **Chat Response**: 18-48 seconds (depending on complexity)
- **Hint Generation**: 15-30 seconds (shorter prompts)
- **Concurrent Users**: Single user (exclusive GPU usage)

### Optimization Features
- Q8_0 quantization (8-bit)
- Vulkan GPU offloading (all layers)
- 120-second timeout for long responses
- Reasoning chain extraction (marker-based)
- Conversation history (last 3 exchanges)
- Rule-based spelling check (80% similarity)

## ğŸ”§ Technical Details

### Model Configuration

**GPT-OSS 20B** (Active):
```python
Model Type: Reasoning model (chain-of-thought)
Generation Parameters:
- max_new_tokens: 512-800 (educational responses)
- temperature: 0.7 (balanced creativity)
- top_p: 0.9
- Reasoning marker: <|start|>assistant<|channel|>final<|message|
- Context: 2048 tokens
- Stop sequences: ["User:", "\nUser:", "Human:", "\nHuman:"]
```

**Llama 3.1 8B Instruct** (Ready for deployment):
```python
Model Type: Direct instruction-following
Generation Parameters:
- max_new_tokens: 512
- temperature: 0.7
- top_p: 0.9
- Chat template: Llama 3 format (<|begin_of_text|>, <|eot_id|>)
- Context: 4096 tokens (supports up to 128K)
- Stop sequences: ["<|eot_id|>", "<|end_of_text|>"]
```

### Answer Evaluation Flow
1. **Spelling Check** (Rule-based): 80%+ similarity â†’ spelling hint
2. **AI Categorization** (GPT-OSS 20B): Related vs Unrelated
3. **Related**: Explain difference + give pedagogical clue
4. **Unrelated**: Close quiz, return to reading

## ğŸ“ Project Structure

```
xilov5/
â”œâ”€â”€ app.py                     # Flask API server (main application)
â”œâ”€â”€ config.py                  # Configuration (v5.09.00)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gptoss_model.py        # GPT-OSS 20B wrapper (Vulkan + reasoning extraction)
â”‚   â”œâ”€â”€ llama31_model.py       # Llama 3.1 8B wrapper (ready for deployment)
â”‚   â”œâ”€â”€ phi_model.py           # Phi 3.5 (deactivated - IPEX-LLM issues)
â”‚   â””â”€â”€ mistral_7b.py          # Mistral 7B (deactivated - IPEX-LLM issues)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ intel_gpu.py          # Intel XPU utilities
â”‚   â”œâ”€â”€ answer_evaluator.py  # Answer evaluation + hints
â”‚   â”œâ”€â”€ lesson_manager.py    # Lesson loading/querying
â”‚   â”œâ”€â”€ progress_tracker.py  # Student progress tracking
â”‚   â””â”€â”€ language_support.py  # Multilingual support (13 languages)
â”œâ”€â”€ lessons/                   # Lesson JSON files
â”‚   â””â”€â”€ grade_X/subject/
â”œâ”€â”€ user_progress/             # Student progress files
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ lessons.css       # Lesson UI styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ lessons.js        # Lesson frontend logic
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html            # Landing page
â”‚   â””â”€â”€ lessons.html          # Lesson viewer
â””â”€â”€ README.md                 # This file
```

## ğŸ› Troubleshooting

### Common Issues

**Models not loading:**
```bash
# Check if both models are cached
ls ~/.cache/huggingface/hub/
```

**High VRAM usage:**
- Both models use ~10-14 GB total
- Close other GPU applications
- Ensure 4-bit quantization is enabled

**Inaccurate hints:**
- Mistral 7B should handle evaluation (check logs)
- Verify Mistral loaded successfully
- Check temperature settings (should be 0.1)

**Quiz not closing on wrong answers:**
- Check backend logs for "should_retry_section" flag
- Verify trigger phrase detection in app.py

## ğŸ“ˆ Version History

### v5.09.00 - Final v5 Release (Model Architecture Overhaul)
- âœ… Integrated GPT-OSS 20B reasoning model with marker-based answer extraction
- âœ… Added Llama 3.1 8B Instruct support (modular design)
- âœ… Implemented reasoning chain extraction (`<|start|>assistant<|channel|>final<|message|>` marker)
- âœ… Migrated from IPEX-LLM to llama.cpp + Vulkan for stability
- âœ… Deactivated Phi 3.5 Mini and Mistral 7B (IPEX-LLM compatibility issues)
- âœ… Disabled Flask auto-reload to prevent model operation interruptions
- âœ… Updated prompt engineering for reasoning models
- âœ… Fixed timeout issues (60s â†’ 120s for long responses)
- âœ… Modular model architecture for easy model swapping
- ğŸ“ Last major release before v6 architectural redesign

### v5.02.00 - Dual-Model System
- âœ… Added Mistral 7B for answer evaluation
- âœ… Dual-model architecture (Phi chat + Mistral evaluation)
- âœ… Improved hint accuracy and consistency
- âœ… Rule-based spelling check (80% similarity)
- âœ… Auto-close quiz on unrelated answers

### v5.01.00 - Lesson System
- âœ… 3-phase lesson flow
- âœ… Multilingual doubt chat (13 languages)
- âœ… Progress tracking system
- âœ… AI-powered hints
- âœ… Split-view interface

### v5.00.00 - Initial Release
- âœ… Intel XPU optimization
- âœ… Phi 3.5 Mini integration
- âœ… Web interface
- âœ… Intel Arc B580 support

## ğŸ”’ License & Copyright

**Copyright Â© 2025 Joseph-Babu**

**All Rights Reserved**

This software is proprietary and confidential. This software is licensed for **personal use only**. 

**Restrictions:**
- No commercial use permitted
- No redistribution allowed
- No modification for commercial purposes
- No open source licensing

**Personal Use License:**
- You may use this software for personal, non-commercial purposes with permission from the author
- You may modify the software for your own personal use
- You may not distribute, share, or sell this software or any derivatives

For licensing inquiries, commercial use requests, or personal use permission, please contact the author.

## ğŸ‘¨â€ğŸ’» Author

**Joseph**
- Lead Developer
- AI/ML Developer  
- Intel XPU Expert

---

## ğŸ™ Acknowledgments

- **OpenAI** for the GPT-OSS open-weight reasoning models
- **Meta** for the Llama 3.1 model family
- **Intel GPU Team** for XPU acceleration technology and Arc GPU development
- **Intel AI Playground Team** for AI Playground and model distribution infrastructure
- **ggerganov** and the llama.cpp community for efficient local inference
- Microsoft for the Phi 3.5 model family (v5.0-5.2)
- Mistral AI for Mistral 7B models (v5.1-5.2)
- PyTorch team for XPU integration
- Hugging Face for model hosting and Transformers library

---

**Xilo AI Tutor v5.09.00** - Powered by Intel Arc GPU (Vulkan) & llama.cpp
